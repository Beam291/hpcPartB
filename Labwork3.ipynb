{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labwork 3\n",
    "This new labwork focuses on the SCAN pattern, which is the Swiss-knife of the little parallel programmer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "In this exercise, you must implement a **block-scan**. \n",
    "It is a scan into a single block, for a small amount of data (256 values). \n",
    "Then, you should implement the PRAM algorithm, with the correct per-warp synchronizations...\n",
    "\n",
    "NB: you must write the three device functions:\n",
    "- `load_shared_memory`\n",
    "- `pointer_jumping`\n",
    "- `save_shared_memory`\n",
    "The kernel should be read, too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel computation time is 189.085693359375 ms\n",
      "- seems to work\n",
      "kernel computation time is 173.2584991455078 ms\n",
      "- seems to work\n",
      "kernel computation time is 260.40228271484375 ms\n",
      "- seems to work\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from numba.np.numpy_support import from_dtype\n",
    "from numba import cuda, core\n",
    "\n",
    "\n",
    "class BlockScan(object):\n",
    "    \"\"\"Create a scan object that scans values using a given binary\n",
    "    function. The binary function is compiled once and cached inside this\n",
    "    object. Keeping this object alive will prevent re-compilation.\n",
    "\n",
    "    The scan is functional for a limited number of values, as it runs into a single block.\n",
    "    THe number of values should be less than 256.\n",
    "    \"\"\"\n",
    "\n",
    "    _cache = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory(fn, np_type):\n",
    "        \"\"\"Factory of kernels for the block-scan problem...\n",
    "\n",
    "        This function returns a Cuda Kernel that does block-scan of some data using a given binary functor.\"\"\"\n",
    "\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        max_block_size = BlockScan._NUM_WARPS * BlockScan._WARP_SIZE\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(shared_memory, d_input):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            # TODO: load data into shared memory\n",
    "            if local_tid < d_input.size:\n",
    "                shared_memory[local_tid] = d_input[local_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(shared_memory, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "            if tid+ jump < cuda.blockDim.x:\n",
    "                temp = shared_memory[tid+ jump]\n",
    "            cuda.syncthreads()\n",
    "            if tid + jump < cuda.blockDim.x:\n",
    "                shared_memory[tid + jump] = scan_op(shared_memory[tid], temp)\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory(shared_memory, d_input):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            # TODO: save data from shared memory\n",
    "            if local_tid < d_input.size:\n",
    "                d_input[local_tid] = shared_memory[local_tid]\n",
    "\n",
    "        def gpu_scan_block(d_input):\n",
    "            \"\"\"\n",
    "            Per block SCAN...\n",
    "            \"\"\"\n",
    "            # move data to shared memory\n",
    "            shared_memory = cuda.shared.array(shape=max_block_size, dtype=np_type)\n",
    "            load_shared_memory(shared_memory, d_input)\n",
    "            \n",
    "            # TODO: implements the logics\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(shared_memory, jump) \n",
    "                jump *= 2\n",
    "                \n",
    "            save_shared_memory(shared_memory, d_input)\n",
    "\n",
    "        return cuda.jit(gpu_scan_block)\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        \"\"\"\n",
    "        :param functor: A function implementing a binary operation for\n",
    "                        scan. It will be compiled as a CUDA device\n",
    "                        function using ``cuda.jit(device=True)``.\n",
    "        \"\"\"\n",
    "        self._functor = functor\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = BlockScan._gpu_kernel_factory(self._functor, from_dtype(dtype))\n",
    "        return self._cache[key]\n",
    "\n",
    "    def __call__(self, d_input, res=None, stream=cuda.default_stream()):\n",
    "        \"\"\" Performs a per-block SCAN.\n",
    "\n",
    "        :param d_input: A host or device array.\n",
    "        :param stream: Optional CUDA stream in which to perform the scan.\n",
    "                    If no stream is specified, the default stream of 0 is used.\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure 1d array\n",
    "        if d_input.ndim != 1:\n",
    "            raise TypeError(\"only support 1D array\")\n",
    "\n",
    "        # ensure size > 0\n",
    "        if d_input.size < 1:\n",
    "            raise ValueError(\"array's length is 0\")\n",
    "\n",
    "        # ensure size < 256\n",
    "        max_block_size = BlockScan._WARP_SIZE * BlockScan._NUM_WARPS\n",
    "        if d_input.size < max_block_size:\n",
    "            raise ValueError(\"array's length is greater than max block size\")\n",
    "\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = core.config.CUDA_LOW_OCCUPANCY_WARNINGS, False\n",
    "\n",
    "        kernel = self._compile(d_input.dtype)\n",
    "\n",
    "        # Perform the reduction on the GPU\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "        nb_threads = max_block_size\n",
    "\n",
    "        nb_blocks = 1\n",
    "\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input)\n",
    "\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "        ct = cuda.event_elapsed_time(start_event, stop_event)\n",
    "        print(f\"kernel computation time is {ct} ms\")\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return d_input.copy_to_host()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def iscan(array, zero):\n",
    "        sum = zero\n",
    "        for i in range(array.size):\n",
    "            sum += array[i]\n",
    "            array[i] = sum\n",
    "\n",
    "\n",
    "    def display(array):\n",
    "        for i in range(array.size):\n",
    "            print(f\"{array[i]} \", end=\"\")\n",
    "            if (i % 16) == 15:\n",
    "                print(\"\")\n",
    "\n",
    "\n",
    "    def check(expected, result):\n",
    "        it_works = True\n",
    "        for i in range(result.size):\n",
    "            if expected[i] != result[i]:\n",
    "                it_works = False\n",
    "        if it_works:\n",
    "            print(f'- seems to work')\n",
    "        else:\n",
    "            print(f'- seems to not work, here is your result:')\n",
    "            display(result)\n",
    "\n",
    "\n",
    "    def test_int32(size):\n",
    "        scanner = BlockScan(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.int32)\n",
    "        result = scanner(cuda.to_device(h_array))\n",
    "        iscan(h_array, np.int32(0))\n",
    "        check(h_array, result)\n",
    "\n",
    "\n",
    "    def test_float32(size):\n",
    "        scanner = BlockScan(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.float32)\n",
    "        result = scanner(cuda.to_device(h_array))\n",
    "        iscan(h_array, np.float32(0))\n",
    "        check(h_array, result)\n",
    "\n",
    "\n",
    "    def test_float64(size):\n",
    "        scanner = BlockScan(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.float64)\n",
    "        result = scanner(cuda.to_device(h_array))\n",
    "        iscan(h_array, np.float64(0))\n",
    "        check(h_array, result)\n",
    "\n",
    "\n",
    "    test_int32(1 << 8)\n",
    "    test_float32(1 << 8)\n",
    "    test_float64(1 << 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "This exercise deals with a block SCAN, but here applying the **hybrid** strategy saw in lecture 3. \n",
    "\n",
    "The idea is then (for a single block):\n",
    "- to load the data in shared memory\n",
    "- to do a SCAN per warp (so without warp synchronization)\n",
    "- to do a warp synchronization\n",
    "- to do a SCAN considering the last value of each warp (into a single warp, so no warp synchronization)\n",
    "- to do a warp synchronization\n",
    "- to apply the final MAP for all warps except the first...\n",
    "\n",
    "Try this wonderful strategy below.\n",
    "\n",
    "NB: the computation time will be roughly speaking the same, but the idea is to apply this algorithm in simple condition first, before to do it at a larger scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel computation time is 0.09200000017881393 ms\n",
      "- seems to not work, here is your result:\n",
      "32 32 32 32 32 32 32 32 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \n",
      "kernel computation time is 0.08848000317811966 ms\n",
      "- seems to not work, here is your result:\n",
      "32.0 32.0 32.0 32.0 32.0 32.0 32.0 32.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "kernel computation time is 0.07942400127649307 ms\n",
      "- seems to not work, here is your result:\n",
      "32.0 32.0 32.0 32.0 32.0 32.0 32.0 32.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "2.000000473111868 512.0001225471497 8192.001972198486 131072.0316772461 524288.1270751953 2097152.5092773438 8388610.041015625 33554440.1796875 67108880.3828125 134217760.796875 268435521.65625 536871043.4375 1073742087.125 2147484174.75 4294968350.5 8589936703.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n",
      "1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 \n",
      "17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0 31.0 32.0 \n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from numba.np.numpy_support import from_dtype\n",
    "from numba import cuda, core\n",
    "\n",
    "\n",
    "class BlockScanHybrid(object):\n",
    "    \"\"\"Create a scan object that scans values using a given binary\n",
    "    function. The binary function is compiled once and cached inside this\n",
    "    object. Keeping this object alive will prevent re-compilation.\n",
    "\n",
    "    The scan is functional for a limited number of values, as it runs into a single block.\n",
    "    THe number of values should be less than 256.\n",
    "    \"\"\"\n",
    "\n",
    "    _cache = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory(fn, np_type):\n",
    "        \"\"\"Factory of kernels for the block-scan problem...\n",
    "\n",
    "        This function returns a Cuda Kernel that does block-scan of some data using a given binary functor.\"\"\"\n",
    "\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        max_block_size = BlockScanHybrid._NUM_WARPS * BlockScanHybrid._WARP_SIZE\n",
    "        \n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(shared_memory, d_input):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            # TODO: load data into shared memory\n",
    "            if local_tid < d_input.size:\n",
    "                shared_memory[local_tid] = d_input[local_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def scan_per_warp(shared_memory):\n",
    "            tid = cuda.threadIdx.x\n",
    "            warp_tid = tid & 31\n",
    "            # TODO: per warp scan\n",
    "            jump = 1\n",
    "            while jump <32:\n",
    "                if warp_tid + jump <32:\n",
    "                    temp = shared_memory[tid]\n",
    "                cuda.syncwarp()\n",
    "                if warp_tid + jump <32:\n",
    "                    shared_memory[tid+jump] = scan_op(temp, shared_memory[tid + jump])\n",
    "                cuda.syncwarp()\n",
    "                jump *= 2\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def get_extra_value(shared_memory, last):\n",
    "            tid = cuda.threadIdx.x\n",
    "            warp_tid = tid & 31\n",
    "            # TODO: collect last value of each warp (copy into \"last\")\n",
    "            if warp_tid == 31:\n",
    "                warp_id = tid >> 5\n",
    "                last[warp_id] = shared_memory[tid]\n",
    "            cuda.syncthreads()\n",
    "            \n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def apply_final_map(shared_memory, last):\n",
    "            tid = cuda.threadIdx.x\n",
    "            warp_id = tid >> 5\n",
    "            # TODO: apply final MAP\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory(shared_memory, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            # TODO: save data from shared memory\n",
    "            if local_tid < d_output.size:\n",
    "                d_output[local_tid] = shared_memory[local_tid]\n",
    "\n",
    "        def gpu_scan_block(d_input, d_output):\n",
    "            \"\"\"\n",
    "            Per block SCAN...\n",
    "            \"\"\"\n",
    "            # move data to shared memory\n",
    "            shared_memory = cuda.shared.array(shape=max_block_size, dtype=np_type)\n",
    "            extra_shared_memory = cuda.shared.array(shape=32, dtype=np_type)\n",
    "            load_shared_memory(shared_memory, d_input)\n",
    "\n",
    "            # TODO: implements the logics\n",
    "            scan_per_warp(shared_memory)\n",
    "            cuda.syncwarp()\n",
    "            get_extra_value(shared_memory, extra_shared_memory)\n",
    "            save_shared_memory(extra_shared_memory, d_output)\n",
    "#             get_extra_value(shared_memory, extra_shared_memory)\n",
    "        return cuda.jit(gpu_scan_block)\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        \"\"\"\n",
    "        :param functor: A function implementing a binary operation for\n",
    "                        scan. It will be compiled as a CUDA device\n",
    "                        function using ``cuda.jit(device=True)``.\n",
    "        \"\"\"\n",
    "        self._functor = functor\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = BlockScanHybrid._gpu_kernel_factory(self._functor, from_dtype(dtype))\n",
    "        return self._cache[key]\n",
    "\n",
    "    def __call__(self, d_input, d_output=None, stream=cuda.default_stream()):\n",
    "        \"\"\" Performs a per-block SCAN.\n",
    "\n",
    "        :param d_input: A host or device array.\n",
    "        :param stream: Optional CUDA stream in which to perform the scan.\n",
    "                    If no stream is specified, the default stream of 0 is used.\n",
    "        \"\"\"\n",
    "        # ensure 1d array\n",
    "        if d_input.ndim != 1:\n",
    "            raise TypeError(\"only support 1D array\")\n",
    "\n",
    "        # ensure size > 0\n",
    "        if d_input.size < 1:\n",
    "            raise ValueError(\"array's length is 0\")\n",
    "\n",
    "        # ensure arrays' size are the same\n",
    "        if d_input.size != d_output.size:\n",
    "            raise ValueError(\"arrays' length are different ({d_input.size} / {d_output.size}\")\n",
    "\n",
    "        # ensure size < 256\n",
    "        max_block_size = BlockScanHybrid._WARP_SIZE * BlockScanHybrid._NUM_WARPS\n",
    "        if d_input.size < max_block_size:\n",
    "            raise ValueError(\"array's `length is greater than max block size\")\n",
    "\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = core.config.CUDA_LOW_OCCUPANCY_WARNINGS, False\n",
    "\n",
    "        kernel = self._compile(d_input.dtype)\n",
    "        # turn on GPU...\n",
    "        nb_threads = max_block_size\n",
    "        nb_blocks = 1\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input, d_output)\n",
    "\n",
    "        # Perform the reduction on the GPU\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        nb_threads = max_block_size\n",
    "        nb_blocks = 1\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input, d_output)\n",
    "\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "        ct = cuda.event_elapsed_time(start_event, stop_event)\n",
    "        print(f\"kernel computation time is {ct} ms\")\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return d_output.copy_to_host()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def iscan(array, zero):\n",
    "        sum = zero\n",
    "        for i in range(array.size):\n",
    "            sum += array[i]\n",
    "            array[i] = sum\n",
    "\n",
    "\n",
    "    def display(array):\n",
    "        for i in range(array.size):\n",
    "            print(f\"{array[i]} \", end=\"\")\n",
    "            if (i % 16) == 15:\n",
    "                print(\"\")\n",
    "\n",
    "\n",
    "    def check(expected, result):\n",
    "        it_works = True\n",
    "        for i in range(result.size):\n",
    "            if expected[i] != result[i]:\n",
    "                it_works = False\n",
    "        if it_works:\n",
    "            print(f'- seems to work')\n",
    "        else:\n",
    "            print(f'- seems to not work, here is your result:')\n",
    "            display(result)\n",
    "\n",
    "\n",
    "    def test_int32(size):\n",
    "        scanner = BlockScanHybrid(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.int32)\n",
    "        result = scanner(cuda.to_device(h_array), cuda.to_device(h_array))\n",
    "        iscan(h_array, np.int32(0))\n",
    "        check(h_array, result)\n",
    "\n",
    "\n",
    "    def test_float32(size):\n",
    "        scanner = BlockScanHybrid(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.float32)\n",
    "        result = scanner(cuda.to_device(h_array), cuda.to_device(h_array))\n",
    "        iscan(h_array, np.float32(0))\n",
    "        check(h_array, result)\n",
    "\n",
    "\n",
    "    def test_float64(size):\n",
    "        scanner = BlockScanHybrid(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.float64)\n",
    "        result = scanner(cuda.to_device(h_array), cuda.to_device(h_array))\n",
    "        iscan(h_array, np.float64(0))\n",
    "        check(h_array, result)\n",
    "\n",
    "\n",
    "    test_int32(1 << 8)\n",
    "    test_float32(1 << 8)\n",
    "    test_float64(1 << 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33 >> 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "In this exercise you must implement the **full scan** for a given binary associative operator.\n",
    "\n",
    "The lecture 3 gives you all the clue for this purpose:\n",
    "- the per block SCAN, with its extra value,\n",
    "- the inter-block extra value scan which is a recursive call,\n",
    "- the MAP to end the SCAN by adding the extra block scanned values (with shift). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from numba.np.numpy_support import from_dtype\n",
    "from numba import cuda, core\n",
    "\n",
    "\n",
    "def display(array, all_values=False):\n",
    "    def aprint(data_range, noend=False):\n",
    "        for i in data_range:\n",
    "            print(f\"{array[i]} \", end=\"\")\n",
    "            if (i % 16) == 15: print(\"\")\n",
    "        if not noend: print(\"...\")\n",
    "\n",
    "    if all_values:\n",
    "        aprint(range(array.size), noend=True)\n",
    "        return\n",
    "    aprint(range(0, min(array.size, 16)))\n",
    "    if array.size < 256 - 16: return\n",
    "    aprint(range(256 - 16, min(array.size, 256 + 16)))\n",
    "    if array.size < 512 - 16: return\n",
    "    aprint(range(512 - 16, min(array.size, 512 + 16)))\n",
    "    if array.size < 768 - 16: return\n",
    "    aprint(range(768 - 16, min(array.size, 768 + 16)))\n",
    "    aprint(range(array.size - 16 - 256, array.size - 256 + 16))\n",
    "    aprint(range(array.size - 16, array.size), noend=True)\n",
    "\n",
    "\n",
    "class InclusiveScan(object):\n",
    "    \"\"\"Create a scan object that scans values using a given binary\n",
    "    function. The binary function is compiled once and cached inside this\n",
    "    object. Keeping this object alive will prevent re-compilation.\n",
    "\n",
    "    The scan is functional for a limited number of values, as it runs into a single block.\n",
    "    THe number of values should be less than 256.\n",
    "    \"\"\"\n",
    "\n",
    "    _kernels_block_scan = {}\n",
    "    _kernels_block_map = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_block_scan_factory(fn, np_type):\n",
    "        \"\"\"Factory of kernels for the block-scan problem...\n",
    "\n",
    "        This function returns a Cuda Kernel that does block-scan of some data using a given binary functor.\"\"\"\n",
    "\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        max_block_size = InclusiveScan._NUM_WARPS * InclusiveScan._WARP_SIZE\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(shared_memory, d_input):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            tid = cuda.grid(1)\n",
    "            # TODO: load data into shared memory\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(shared_memory, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "            # TODO: implement the pointer jumping (full block)\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory(shared_memory, d_output, d_extras):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            # TODO: save data from shared memory\n",
    "            \n",
    "            # TODO: save last block value to \"d_extras\"!\n",
    "\n",
    "        def gpu_scan_block(d_input, d_output, d_extras):\n",
    "            \"\"\"\n",
    "            Per block SCAN...\n",
    "            \"\"\"\n",
    "            # move data to shared memory\n",
    "            shared_memory = cuda.shared.array(shape=max_block_size, dtype=np_type)\n",
    "            load_shared_memory(shared_memory, d_input)\n",
    "\n",
    "            # TODO: implements the logics\n",
    "\n",
    "            # now stores the result\n",
    "            save_shared_memory(shared_memory, d_output, d_extras)\n",
    "\n",
    "        return cuda.jit(gpu_scan_block)\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        \"\"\"\n",
    "        :param functor: A function implementing a binary operation for\n",
    "                        scan. It will be compiled as a CUDA device\n",
    "                        function using ``cuda.jit(device=True)``.\n",
    "        \"\"\"\n",
    "        self._functor = functor\n",
    "\n",
    "    def _compile_block_scan(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._kernels_block_scan:\n",
    "            self._kernels_block_scan[key] = InclusiveScan._gpu_kernel_block_scan_factory(self._functor,\n",
    "                                                                                         from_dtype(dtype))\n",
    "        return self._kernels_block_scan[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_block_map_factory(fn, np_type):\n",
    "        \"\"\"Factory of kernels for the block-map problem...\n",
    "\n",
    "        This function returns a Cuda Kernel that does block-map of some data using a given binary functor.\"\"\"\n",
    "\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        def gpu_map_block(d_io, d_scan):\n",
    "            \"\"\"\n",
    "            Per block MAP...\n",
    "            \"\"\"\n",
    "            # TODO: implements the logics\n",
    "\n",
    "        return cuda.jit(gpu_map_block)\n",
    "\n",
    "    def _compile_block_map(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._kernels_block_map:\n",
    "            self._kernels_block_map[key] = InclusiveScan._gpu_kernel_block_map_factory(self._functor, from_dtype(dtype))\n",
    "        return self._kernels_block_map[key]\n",
    "\n",
    "    def __call__(self, d_input, d_output, stream=cuda.default_stream()):\n",
    "        \"\"\" Performs a per-block SCAN.\n",
    "\n",
    "        :param d_input: A host or device array.\n",
    "        :param stream: Optional CUDA stream in which to perform the scan.\n",
    "                    If no stream is specified, the default stream of 0 is used.\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure 1d array\n",
    "        if d_input.ndim != 1:\n",
    "            raise TypeError(\"only support 1D array\")\n",
    "\n",
    "        # ensure size > 0\n",
    "        if d_input.size < 1:\n",
    "            raise ValueError(\"array's length is 0\")\n",
    "\n",
    "        # ensure arrays' size are the same\n",
    "        if d_input.size != d_output.size:\n",
    "            raise ValueError(\"arrays' length are different ({d_input.size} / {d_output.size}\")\n",
    "\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = core.config.CUDA_LOW_OCCUPANCY_WARNINGS, False\n",
    "\n",
    "        kernel_step1 = self._compile_block_scan(d_input.dtype)\n",
    "        kernel_step2 = self._compile_block_map(d_input.dtype)\n",
    "\n",
    "        max_block_size = InclusiveScan._WARP_SIZE * InclusiveScan._NUM_WARPS\n",
    "        nb_threads = min(max_block_size, d_input.size)\n",
    "        nb_blocks = (d_input.size + nb_threads - 1) // nb_threads\n",
    "        extras = cuda.device_array(shape=nb_blocks, dtype=d_input.dtype)\n",
    "\n",
    "        # Perform the reduction on the GPU\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        # TODO implement the logics\n",
    "\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        # display(extras)\n",
    "\n",
    "        return cuda.event_elapsed_time(start_event, stop_event)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def check(expected, result):\n",
    "        it_works = np.array_equal(expected, result)\n",
    "        if it_works:\n",
    "            print(f'- seems to work')\n",
    "        else:\n",
    "            print(f'- seems to not work, here is your result:')\n",
    "            display(result)\n",
    "            display(expected)\n",
    "\n",
    "\n",
    "    def test_int32(size):\n",
    "        scanner = InclusiveScan(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.int32)\n",
    "        d_result = cuda.device_array(size, dtype=np.int32)\n",
    "        scanner(cuda.to_device(h_array), d_result)  # turn on gpu\n",
    "        ct = scanner(cuda.to_device(h_array), d_result)\n",
    "        print(f\"scan computation time is {ct} ms\")\n",
    "        expected = np.cumsum(h_array)\n",
    "        check(expected, d_result.copy_to_host())\n",
    "\n",
    "\n",
    "    def test_float32(size):\n",
    "        scanner = InclusiveScan(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.float32)\n",
    "        d_result = cuda.device_array(size, dtype=np.float32)\n",
    "        scanner(cuda.to_device(h_array), d_result)  # turn on gpu\n",
    "        ct = scanner(cuda.to_device(h_array), d_result)\n",
    "        print(f\"scan computation time is {ct} ms\")\n",
    "        expected = np.cumsum(h_array)\n",
    "        check(expected, d_result.copy_to_host())\n",
    "\n",
    "\n",
    "    def test_float64(size):\n",
    "        scanner = InclusiveScan(lambda a, b: a + b)\n",
    "        h_array = np.ones(size, dtype=np.float64)\n",
    "        d_result = cuda.device_array(size, dtype=np.float64)\n",
    "        scanner(cuda.to_device(h_array), d_result)  # turn on gpu\n",
    "        ct = scanner(cuda.to_device(h_array), d_result)\n",
    "        print(f\"scan computation time is {ct} ms\")\n",
    "        expected = np.cumsum(h_array)\n",
    "        check(expected, d_result.copy_to_host())\n",
    "\n",
    "\n",
    "    test_int32(1 << 24)\n",
    "    # with float32, the sequential cumsum fails with more than 2^24 number...\n",
    "    test_float32(1 << 24)\n",
    "    test_float64(1 << 24)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b4d75ac280b6c7c3aa43866cb82dc88915409b55fec83a093dd0284cb58708e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
